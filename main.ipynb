{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from etl.extract_data import extract_data\n",
    "from etl.transform_data import clean_data, format_column_name\n",
    "from etl.load_data import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Spark + Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Delta Lake setup is successful!\n",
      "<pyspark.sql.session.SparkSession object at 0x0000029B2B1DBA90>\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark + Delta Lake ( define DeltaSparkSessionExtension and DeltaCatalog)\n",
    "builder = SparkSession.builder.appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "print(\"Spark + Delta Lake setup is successful!\")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Transform data from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+-----------------------+-------------+------------+--------------------+-------------+--------+--------------+--------------------+-----------------------+---------------------+------------+\n",
      "|Tourist_ID|Age|           Interests|Preferred_Tour_Duration|Accessibility|   Site_Name|       Sites_Visited|Tour_Duration|Route_ID|Tourist_Rating|System_Response_Time|Recommendation_Accuracy|VR_Experience_Quality|Satisfaction|\n",
      "+----------+---+--------------------+-----------------------+-------------+------------+--------------------+-------------+--------+--------------+--------------------+-----------------------+---------------------+------------+\n",
      "|         1| 48|['Architecture', ...|                      5|        False|Eiffel Tower|['Eiffel Tower', ...|            7|    1000|           1.6|                3.73|                     97|                  4.5|           3|\n",
      "|         2| 37|['Cultural', 'Nat...|                      6|        False|   Colosseum|['Great Wall of C...|            1|    2000|           2.6|                2.89|                     90|                  4.5|           3|\n",
      "|         3| 43|['History', 'Art'...|                      6|         True|Machu Picchu|    ['Eiffel Tower']|            2|    3000|           1.7|                2.22|                     94|                  4.7|           3|\n",
      "|         4| 46|['Cultural', 'Art...|                      8|        False|   Colosseum|['Machu Picchu', ...|            5|    4000|           2.0|                2.34|                     92|                  4.7|           3|\n",
      "|         5| 53|['Architecture', ...|                      5|         True|   Colosseum|['Machu Picchu', ...|            7|    5000|           3.7|                 2.0|                     96|                  4.8|           4|\n",
      "+----------+---+--------------------+-----------------------+-------------+------------+--------------------+-------------+--------+--------------+--------------------+-----------------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract data from a file .csv\n",
    "df = extract_data(spark, \"data/tourism_dataset.csv\")\n",
    "df_clean = clean_data(df)\n",
    "df_formatted = format_column_name(df_clean)\n",
    "new_df = df_formatted\n",
    "try: \n",
    "    new_df.show(5)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to a Delta table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a Delta table\n",
    "load_data(new_df, \"data/delta_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o93.load.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat io.delta.storage.HadoopFileSystemLogStore.listFrom(HadoopFileSystemLogStore.java:59)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreAdaptor.listFrom(LogStore.scala:452)\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.listFrom(DelegatingLogStore.scala:127)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFrom(SnapshotManagement.scala:86)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFrom$(SnapshotManagement.scala:85)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.listFrom(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFromOrNone(SnapshotManagement.scala:103)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFromOrNone$(SnapshotManagement.scala:99)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.listFromOrNone(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFromFileSystemInternal(SnapshotManagement.scala:113)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$listDeltaCompactedDeltaAndCheckpointFiles$2(SnapshotManagement.scala:158)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$listDeltaCompactedDeltaAndCheckpointFiles$1(SnapshotManagement.scala:158)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listDeltaCompactedDeltaAndCheckpointFiles(SnapshotManagement.scala:156)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listDeltaCompactedDeltaAndCheckpointFiles$(SnapshotManagement.scala:151)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.listDeltaCompactedDeltaAndCheckpointFiles(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.createLogSegment(SnapshotManagement.scala:302)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.createLogSegment$(SnapshotManagement.scala:290)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.createLogSegment(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.createLogSegment(SnapshotManagement.scala:314)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$updateInternal$1(SnapshotManagement.scala:984)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.updateInternal(SnapshotManagement.scala:981)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.updateInternal$(SnapshotManagement.scala:980)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.updateInternal(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$update$3(SnapshotManagement.scala:932)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$update$2(SnapshotManagement.scala:931)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.update(SnapshotManagement.scala:931)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.update$(SnapshotManagement.scala:899)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.update(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:147)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from a Delta table\n",
    "try:\n",
    "    df_delta = spark.read.format(\"delta\").load(\"data/delta_table\")\n",
    "    df_delta.show(5)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
